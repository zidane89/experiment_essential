{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5103.396530786437 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 495.0200 Fuel Consumption: 153.1967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5056.339657396753 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 490.9893 Fuel Consumption: 146.4470\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5134.7220476725515 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 498.7376 Fuel Consumption: 147.3463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5138.266307604152 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 498.6477 Fuel Consumption: 151.7891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5006.747830090441 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 485.9226 Fuel Consumption: 147.5223\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5086.704995728649 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 493.6793 Fuel Consumption: 149.9123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5129.297091769225 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 497.8740 Fuel Consumption: 150.5575\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5077.586609133319 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 492.5714 Fuel Consumption: 151.8722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -5060.389503498156 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 491.4773 Fuel Consumption: 145.6162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -5167.221664904731 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 501.3067 Fuel Consumption: 154.1547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -5112.8126500339395 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 496.1576 Fuel Consumption: 151.2368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -5052.746476320832 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 490.5708 Fuel Consumption: 147.0384\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -5050.94055491222 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 490.2563 Fuel Consumption: 148.3776\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -5114.935412822691 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 496.7677 Fuel Consumption: 147.2584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5092.796182595123 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 494.0909 Fuel Consumption: 151.8869\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -5050.812166029797 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 490.6760 Fuel Consumption: 144.0521\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5006.8970574106215 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 486.4942 Fuel Consumption: 141.9554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -5136.83736438262 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 499.0273 Fuel Consumption: 146.5643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4946.71160819789 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 480.4795 Fuel Consumption: 141.9163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -5125.839030980609 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 497.6914 Fuel Consumption: 148.9250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -5127.638312254601 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 498.0453 Fuel Consumption: 147.1851\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4988.090852196362 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 484.6673 Fuel Consumption: 141.4178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -5160.920243111007 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 501.0788 Fuel Consumption: 150.1322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -5177.921300581719 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 502.2941 Fuel Consumption: 154.9807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -5072.128792817346 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 492.1415 Fuel Consumption: 150.7139\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -5179.720473646711 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 501.7394 Fuel Consumption: 162.3267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -5200.313508367187 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 504.4096 Fuel Consumption: 156.2172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -5145.420347152933 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 498.8426 Fuel Consumption: 156.9944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -5199.197414721339 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 504.8816 Fuel Consumption: 150.3815\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4987.404192211375 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 484.3880 Fuel Consumption: 143.5243\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -5018.866326455803 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 488.1007 Fuel Consumption: 137.8596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4956.62710657613 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 481.2634 Fuel Consumption: 143.9935\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -4938.141816939804 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 480.0473 Fuel Consumption: 137.6690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4976.387490160986 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 483.4755 Fuel Consumption: 141.6328\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -5102.536142049191 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 494.8601 Fuel Consumption: 153.9348\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -5139.9683516216 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 498.3116 Fuel Consumption: 156.8526\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -5162.860403773481 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 501.4356 Fuel Consumption: 148.5047\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -5207.422428591068 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 506.3331 Fuel Consumption: 144.0912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -5084.666734448771 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 492.3767 Fuel Consumption: 160.8995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -4910.205939342771 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 476.3651 Fuel Consumption: 146.5546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -4829.661359899476 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 468.7750 Fuel Consumption: 141.9114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -5032.432405379155 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 487.5814 Fuel Consumption: 156.6180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -5053.646485253054 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 490.4663 Fuel Consumption: 148.9837\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -5041.666476839984 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 489.6045 Fuel Consumption: 145.6210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -5011.3430340450695 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 485.9011 Fuel Consumption: 152.3316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -5081.183312465505 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 491.9023 Fuel Consumption: 162.1605\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -4967.250508247498 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 481.4621 Fuel Consumption: 152.6298\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -5056.2617336493595 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 490.0475 Fuel Consumption: 155.7871\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -5030.5744249590125 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 488.3966 Fuel Consumption: 146.6083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -4953.188467047871 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 480.4884 Fuel Consumption: 148.3043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -4948.146001195726 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 480.0399 Fuel Consumption: 147.7471\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4985.703580455746 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 483.9447 Fuel Consumption: 146.2564\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -4998.586593572702 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 485.6411 Fuel Consumption: 142.1753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -4915.8404037052105 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 476.6998 Fuel Consumption: 148.8419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -4975.3323770717425 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 482.1236 Fuel Consumption: 154.0960\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4807.2723323093805 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 466.7199 Fuel Consumption: 140.0737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4840.961551007339 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 470.9710 Fuel Consumption: 131.2517\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -4995.425019810345 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 485.0185 Fuel Consumption: 145.2398\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -4933.154622673485 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 479.3047 Fuel Consumption: 140.1079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -5029.623500918965 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 487.7742 Fuel Consumption: 151.8820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -4969.166483899661 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 481.7783 Fuel Consumption: 151.3835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -4953.270975367968 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 480.1291 Fuel Consumption: 151.9797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -4839.131439358115 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 469.8618 Fuel Consumption: 140.5136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -4961.385496734082 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 480.8687 Fuel Consumption: 152.6982\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -5010.020846966301 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 486.4077 Fuel Consumption: 145.9436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -4873.835786471328 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 473.1651 Fuel Consumption: 142.1851\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -5044.755760343074 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 489.4052 Fuel Consumption: 150.7041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -4764.750336418384 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 463.1627 Fuel Consumption: 133.1236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -4767.248203184736 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 464.0190 Fuel Consumption: 127.0582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -4877.954291011758 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 472.2763 Fuel Consumption: 155.1908\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -4800.908853802967 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 465.5791 Fuel Consumption: 145.1176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -4958.3974917987825 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 480.4253 Fuel Consumption: 154.1449\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -5010.910076356498 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 486.2337 Fuel Consumption: 148.5731\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -4808.737334230168 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 466.6909 Fuel Consumption: 141.8283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -4986.451274363691 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 484.9682 Fuel Consumption: 136.7697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -5062.761625656875 Explore P: 0.1263 SOC: 1.0000 Cumulative_SOC_deviation: 491.0625 Fuel Consumption: 152.1361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -4984.690097340535 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 481.9993 Fuel Consumption: 164.6971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -5103.633196228164 Explore P: 0.1200 SOC: 1.0000 Cumulative_SOC_deviation: 493.6355 Fuel Consumption: 167.2777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -5041.704817427232 Explore P: 0.1171 SOC: 1.0000 Cumulative_SOC_deviation: 488.9280 Fuel Consumption: 152.4245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -5002.667190028753 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 486.2550 Fuel Consumption: 140.1177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -4948.3250099285215 Explore P: 0.1113 SOC: 1.0000 Cumulative_SOC_deviation: 480.7093 Fuel Consumption: 141.2320\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -4831.285480211102 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 469.6603 Fuel Consumption: 134.6827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -4958.640194130074 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 481.7594 Fuel Consumption: 141.0463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -5049.444930061386 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 490.9122 Fuel Consumption: 140.3230\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -4933.195416116154 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 479.1299 Fuel Consumption: 141.8967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -5027.092081938011 Explore P: 0.0983 SOC: 1.0000 Cumulative_SOC_deviation: 487.9839 Fuel Consumption: 147.2535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -4891.467262024919 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 474.1633 Fuel Consumption: 149.8341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -4977.686498723337 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 482.1196 Fuel Consumption: 156.4909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -4863.436789347746 Explore P: 0.0914 SOC: 1.0000 Cumulative_SOC_deviation: 472.2957 Fuel Consumption: 140.4794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -4933.398977559929 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 477.3751 Fuel Consumption: 159.6483\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -5022.341805588124 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 487.9257 Fuel Consumption: 143.0844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -4990.005593346232 Explore P: 0.0849 SOC: 1.0000 Cumulative_SOC_deviation: 484.0587 Fuel Consumption: 149.4187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -4979.629333877194 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 484.0162 Fuel Consumption: 139.4676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -4775.642970682206 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 463.5046 Fuel Consumption: 140.5967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -4945.059615299952 Explore P: 0.0790 SOC: 1.0000 Cumulative_SOC_deviation: 479.8422 Fuel Consumption: 146.6377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -4960.42438056823 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 480.5365 Fuel Consumption: 155.0589\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -5214.566402030949 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 504.4801 Fuel Consumption: 169.7655\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -4877.042232513443 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 472.8865 Fuel Consumption: 148.1772\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -4976.967705925471 Explore P: 0.0718 SOC: 1.0000 Cumulative_SOC_deviation: 483.5183 Fuel Consumption: 141.7843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -5046.13387472682 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 489.6314 Fuel Consumption: 149.8194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -4973.628062195193 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 481.8076 Fuel Consumption: 155.5525\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -4821.056974120149 Explore P: 0.0669 SOC: 1.0000 Cumulative_SOC_deviation: 468.0499 Fuel Consumption: 140.5576\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -4981.161316297187 Explore P: 0.0654 SOC: 1.0000 Cumulative_SOC_deviation: 484.0398 Fuel Consumption: 140.7628\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -5001.857809613547 Explore P: 0.0639 SOC: 1.0000 Cumulative_SOC_deviation: 485.6999 Fuel Consumption: 144.8586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -4892.93399999191 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 475.1326 Fuel Consumption: 141.6084\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -4857.398329221099 Explore P: 0.0610 SOC: 1.0000 Cumulative_SOC_deviation: 471.2466 Fuel Consumption: 144.9319\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -5093.759869812917 Explore P: 0.0596 SOC: 1.0000 Cumulative_SOC_deviation: 495.7582 Fuel Consumption: 136.1783\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -5096.699451170376 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 494.2496 Fuel Consumption: 154.2036\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -5038.819948741222 Explore P: 0.0570 SOC: 1.0000 Cumulative_SOC_deviation: 489.3272 Fuel Consumption: 145.5477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -5254.183403288147 Explore P: 0.0557 SOC: 1.0000 Cumulative_SOC_deviation: 510.0693 Fuel Consumption: 153.4900\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -5092.308174993191 Explore P: 0.0545 SOC: 1.0000 Cumulative_SOC_deviation: 492.6335 Fuel Consumption: 165.9728\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -5118.308601590832 Explore P: 0.0533 SOC: 1.0000 Cumulative_SOC_deviation: 496.2287 Fuel Consumption: 156.0217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -4714.955051550163 Explore P: 0.0521 SOC: 1.0000 Cumulative_SOC_deviation: 458.6132 Fuel Consumption: 128.8226\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -4853.525216354846 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 471.6531 Fuel Consumption: 136.9945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -4955.840175181254 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 482.8846 Fuel Consumption: 126.9946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -5240.066136251124 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 508.8893 Fuel Consumption: 151.1733\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -4859.634880185513 Explore P: 0.0477 SOC: 1.0000 Cumulative_SOC_deviation: 470.2988 Fuel Consumption: 156.6473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -4928.926730812861 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 477.2490 Fuel Consumption: 156.4372\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -4865.029373532171 Explore P: 0.0457 SOC: 1.0000 Cumulative_SOC_deviation: 474.0508 Fuel Consumption: 124.5215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -4991.405718308467 Explore P: 0.0447 SOC: 1.0000 Cumulative_SOC_deviation: 485.4915 Fuel Consumption: 136.4911\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -5004.83993279813 Explore P: 0.0438 SOC: 1.0000 Cumulative_SOC_deviation: 487.4971 Fuel Consumption: 129.8685\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -5176.915372605931 Explore P: 0.0429 SOC: 1.0000 Cumulative_SOC_deviation: 502.9965 Fuel Consumption: 146.9505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -4805.6260520362175 Explore P: 0.0420 SOC: 1.0000 Cumulative_SOC_deviation: 468.0132 Fuel Consumption: 125.4942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -4939.309413283949 Explore P: 0.0411 SOC: 1.0000 Cumulative_SOC_deviation: 478.5898 Fuel Consumption: 153.4118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -4761.5191424679315 Explore P: 0.0403 SOC: 1.0000 Cumulative_SOC_deviation: 462.5419 Fuel Consumption: 136.1001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -4644.856745944416 Explore P: 0.0395 SOC: 1.0000 Cumulative_SOC_deviation: 451.5071 Fuel Consumption: 129.7854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -4687.447527729077 Explore P: 0.0387 SOC: 1.0000 Cumulative_SOC_deviation: 456.4632 Fuel Consumption: 122.8158\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -4585.237917569723 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 447.4651 Fuel Consumption: 110.5872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -4725.494889300909 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 460.3686 Fuel Consumption: 121.8090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -3174.6664458943933 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 309.1347 Fuel Consumption: 83.3196\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -3906.400489965921 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 381.6058 Fuel Consumption: 90.3430\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -4708.353506707546 Explore P: 0.0350 SOC: 1.0000 Cumulative_SOC_deviation: 457.9927 Fuel Consumption: 128.4267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -4379.572637132744 Explore P: 0.0343 SOC: 1.0000 Cumulative_SOC_deviation: 426.2060 Fuel Consumption: 117.5128\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -4565.632297210887 Explore P: 0.0336 SOC: 1.0000 Cumulative_SOC_deviation: 445.4180 Fuel Consumption: 111.4523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -4888.884554803526 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 474.8185 Fuel Consumption: 140.6993\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -4408.544418422082 Explore P: 0.0324 SOC: 1.0000 Cumulative_SOC_deviation: 428.9644 Fuel Consumption: 118.9009\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -4330.486730701857 Explore P: 0.0318 SOC: 1.0000 Cumulative_SOC_deviation: 422.8751 Fuel Consumption: 101.7358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -2858.6733760545535 Explore P: 0.0312 SOC: 1.0000 Cumulative_SOC_deviation: 276.2490 Fuel Consumption: 96.1836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -4045.666821554707 Explore P: 0.0306 SOC: 1.0000 Cumulative_SOC_deviation: 394.0827 Fuel Consumption: 104.8394\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -4401.69413085316 Explore P: 0.0301 SOC: 1.0000 Cumulative_SOC_deviation: 427.1689 Fuel Consumption: 130.0054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -5044.411465736803 Explore P: 0.0295 SOC: 1.0000 Cumulative_SOC_deviation: 490.8228 Fuel Consumption: 136.1832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -4769.453503475258 Explore P: 0.0290 SOC: 1.0000 Cumulative_SOC_deviation: 462.9233 Fuel Consumption: 140.2203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -4906.1968848236675 Explore P: 0.0285 SOC: 1.0000 Cumulative_SOC_deviation: 475.1617 Fuel Consumption: 154.5799\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -4813.664352272864 Explore P: 0.0280 SOC: 1.0000 Cumulative_SOC_deviation: 468.0873 Fuel Consumption: 132.7913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -4359.361102320514 Explore P: 0.0275 SOC: 1.0000 Cumulative_SOC_deviation: 423.4595 Fuel Consumption: 124.7659\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -5121.953404565778 Explore P: 0.0270 SOC: 1.0000 Cumulative_SOC_deviation: 496.9578 Fuel Consumption: 152.3756\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -4453.833099759418 Explore P: 0.0265 SOC: 1.0000 Cumulative_SOC_deviation: 431.2958 Fuel Consumption: 140.8752\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -3729.2917879137417 Explore P: 0.0261 SOC: 1.0000 Cumulative_SOC_deviation: 361.9467 Fuel Consumption: 109.8247\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -3562.7120255347368 Explore P: 0.0257 SOC: 1.0000 Cumulative_SOC_deviation: 346.3611 Fuel Consumption: 99.1014\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -3830.71448175343 Explore P: 0.0252 SOC: 1.0000 Cumulative_SOC_deviation: 372.1564 Fuel Consumption: 109.1502\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -3393.6505065495776 Explore P: 0.0248 SOC: 1.0000 Cumulative_SOC_deviation: 329.9940 Fuel Consumption: 93.7105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -3867.3461270209064 Explore P: 0.0244 SOC: 1.0000 Cumulative_SOC_deviation: 376.4027 Fuel Consumption: 103.3194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -3211.2209774142007 Explore P: 0.0240 SOC: 1.0000 Cumulative_SOC_deviation: 311.5648 Fuel Consumption: 95.5726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -3424.486372295005 Explore P: 0.0237 SOC: 1.0000 Cumulative_SOC_deviation: 331.2237 Fuel Consumption: 112.2489\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -3493.1326622768743 Explore P: 0.0233 SOC: 1.0000 Cumulative_SOC_deviation: 338.1412 Fuel Consumption: 111.7211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -4271.6456859083855 Explore P: 0.0229 SOC: 1.0000 Cumulative_SOC_deviation: 416.2354 Fuel Consumption: 109.2920\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -4731.613815800446 Explore P: 0.0226 SOC: 1.0000 Cumulative_SOC_deviation: 461.1789 Fuel Consumption: 119.8246\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -3445.3478853308425 Explore P: 0.0222 SOC: 1.0000 Cumulative_SOC_deviation: 334.8011 Fuel Consumption: 97.3370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -3560.507117435407 Explore P: 0.0219 SOC: 1.0000 Cumulative_SOC_deviation: 345.9969 Fuel Consumption: 100.5384\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -3650.2334179133127 Explore P: 0.0216 SOC: 1.0000 Cumulative_SOC_deviation: 354.3287 Fuel Consumption: 106.9459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -4264.426982925106 Explore P: 0.0213 SOC: 1.0000 Cumulative_SOC_deviation: 414.9250 Fuel Consumption: 115.1766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -3639.993178451829 Explore P: 0.0210 SOC: 1.0000 Cumulative_SOC_deviation: 352.7642 Fuel Consumption: 112.3516\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -3286.4989060507 Explore P: 0.0207 SOC: 1.0000 Cumulative_SOC_deviation: 318.1972 Fuel Consumption: 104.5266\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -3823.149676389457 Explore P: 0.0204 SOC: 1.0000 Cumulative_SOC_deviation: 372.4190 Fuel Consumption: 98.9597\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -1532.465841906395 Explore P: 0.0201 SOC: 0.9215 Cumulative_SOC_deviation: 145.7465 Fuel Consumption: 75.0010\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -1055.7257892742448 Explore P: 0.0198 SOC: 0.7503 Cumulative_SOC_deviation: 99.2699 Fuel Consumption: 63.0265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -2652.582089932859 Explore P: 0.0196 SOC: 1.0000 Cumulative_SOC_deviation: 256.7635 Fuel Consumption: 84.9471\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -2746.6778934135496 Explore P: 0.0193 SOC: 0.9676 Cumulative_SOC_deviation: 267.0607 Fuel Consumption: 76.0714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -2737.5962703234927 Explore P: 0.0190 SOC: 0.8709 Cumulative_SOC_deviation: 266.8763 Fuel Consumption: 68.8329\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -2610.3517070423177 Explore P: 0.0188 SOC: 0.9439 Cumulative_SOC_deviation: 253.4539 Fuel Consumption: 75.8123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -3230.3216788833693 Explore P: 0.0186 SOC: 0.9097 Cumulative_SOC_deviation: 315.8302 Fuel Consumption: 72.0196\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -4101.438152148664 Explore P: 0.0183 SOC: 0.9986 Cumulative_SOC_deviation: 401.0088 Fuel Consumption: 91.3498\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -3448.2309929920425 Explore P: 0.0181 SOC: 0.9966 Cumulative_SOC_deviation: 336.8865 Fuel Consumption: 79.3656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -4282.54141881623 Explore P: 0.0179 SOC: 1.0000 Cumulative_SOC_deviation: 419.0981 Fuel Consumption: 91.5600\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -3532.621351106934 Explore P: 0.0177 SOC: 1.0000 Cumulative_SOC_deviation: 344.3315 Fuel Consumption: 89.3068\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -5062.313317280107 Explore P: 0.0175 SOC: 1.0000 Cumulative_SOC_deviation: 495.6887 Fuel Consumption: 105.4259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -4625.6580951018805 Explore P: 0.0173 SOC: 1.0000 Cumulative_SOC_deviation: 451.9489 Fuel Consumption: 106.1688\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -4615.282448163351 Explore P: 0.0171 SOC: 1.0000 Cumulative_SOC_deviation: 451.1528 Fuel Consumption: 103.7544\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -4326.400305673455 Explore P: 0.0169 SOC: 1.0000 Cumulative_SOC_deviation: 420.6180 Fuel Consumption: 120.2205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -4299.923053775036 Explore P: 0.0167 SOC: 1.0000 Cumulative_SOC_deviation: 418.4595 Fuel Consumption: 115.3281\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -4756.927775164965 Explore P: 0.0165 SOC: 1.0000 Cumulative_SOC_deviation: 465.0510 Fuel Consumption: 106.4181\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -5007.301644271198 Explore P: 0.0163 SOC: 1.0000 Cumulative_SOC_deviation: 488.2966 Fuel Consumption: 124.3358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -4790.160797106202 Explore P: 0.0162 SOC: 1.0000 Cumulative_SOC_deviation: 467.0180 Fuel Consumption: 119.9810\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -4752.463869459108 Explore P: 0.0160 SOC: 1.0000 Cumulative_SOC_deviation: 463.5826 Fuel Consumption: 116.6379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -5136.122757991862 Explore P: 0.0158 SOC: 1.0000 Cumulative_SOC_deviation: 501.3908 Fuel Consumption: 122.2146\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -4684.223177982348 Explore P: 0.0157 SOC: 1.0000 Cumulative_SOC_deviation: 457.2717 Fuel Consumption: 111.5060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -4555.616227137892 Explore P: 0.0155 SOC: 1.0000 Cumulative_SOC_deviation: 445.5635 Fuel Consumption: 99.9812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -4678.997290905902 Explore P: 0.0154 SOC: 1.0000 Cumulative_SOC_deviation: 457.4446 Fuel Consumption: 104.5511\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -4434.686709888385 Explore P: 0.0152 SOC: 1.0000 Cumulative_SOC_deviation: 434.0527 Fuel Consumption: 94.1601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -4619.28185994275 Explore P: 0.0151 SOC: 1.0000 Cumulative_SOC_deviation: 450.1285 Fuel Consumption: 117.9967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -4182.246369500516 Explore P: 0.0149 SOC: 1.0000 Cumulative_SOC_deviation: 407.1063 Fuel Consumption: 111.1834\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -4524.307178075039 Explore P: 0.0148 SOC: 1.0000 Cumulative_SOC_deviation: 440.1042 Fuel Consumption: 123.2654\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -4958.817408514515 Explore P: 0.0147 SOC: 1.0000 Cumulative_SOC_deviation: 483.1329 Fuel Consumption: 127.4883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -3569.2229739805225 Explore P: 0.0146 SOC: 1.0000 Cumulative_SOC_deviation: 347.6685 Fuel Consumption: 92.5375\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -3612.984821399001 Explore P: 0.0144 SOC: 1.0000 Cumulative_SOC_deviation: 351.7138 Fuel Consumption: 95.8463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -3884.0566054700485 Explore P: 0.0143 SOC: 1.0000 Cumulative_SOC_deviation: 379.6314 Fuel Consumption: 87.7428\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -3789.068273229462 Explore P: 0.0142 SOC: 1.0000 Cumulative_SOC_deviation: 370.1453 Fuel Consumption: 87.6157\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -4377.347815729056 Explore P: 0.0141 SOC: 1.0000 Cumulative_SOC_deviation: 427.2816 Fuel Consumption: 104.5315\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_1_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
