{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5078.372010499781 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 492.8543 Fuel Consumption: 149.8292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5116.322185154279 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 496.9557 Fuel Consumption: 146.7647\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5073.015327832334 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 492.2868 Fuel Consumption: 150.1469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5036.542857622103 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 489.2872 Fuel Consumption: 143.6709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5028.269158984206 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 488.7599 Fuel Consumption: 140.6700\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4914.513059894684 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 477.7699 Fuel Consumption: 136.8137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5074.03030155301 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 493.1547 Fuel Consumption: 142.4832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5095.805642402281 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 495.5610 Fuel Consumption: 140.1959\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4939.246134100223 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 480.1797 Fuel Consumption: 137.4491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4949.192841225812 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 481.4207 Fuel Consumption: 134.9858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4915.545798351957 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 478.7080 Fuel Consumption: 128.4658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4956.765872257344 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 482.1184 Fuel Consumption: 135.5820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4925.59669135646 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 479.4770 Fuel Consumption: 130.8265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4929.637587505803 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 479.1998 Fuel Consumption: 137.6397\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4914.913855115464 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 477.8516 Fuel Consumption: 136.3983\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4692.432797268688 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 457.1728 Fuel Consumption: 120.7044\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4785.8424522935375 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 465.8325 Fuel Consumption: 127.5176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4984.205590893676 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 485.8394 Fuel Consumption: 125.8118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4828.354467609143 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 470.0715 Fuel Consumption: 127.6398\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4692.378909451273 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 457.2154 Fuel Consumption: 120.2254\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4706.337765662145 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 458.9099 Fuel Consumption: 117.2391\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4865.725723583363 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 474.1942 Fuel Consumption: 123.7835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4787.752027968967 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 466.3841 Fuel Consumption: 123.9106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4793.0454447957345 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 466.3974 Fuel Consumption: 129.0718\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4665.2689484265375 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 453.0943 Fuel Consumption: 134.3259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4487.07883417376 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 436.0030 Fuel Consumption: 127.0484\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4551.5177415970475 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 442.5095 Fuel Consumption: 126.4228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4671.512062298653 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 454.5299 Fuel Consumption: 126.2126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4863.549811903584 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 471.3281 Fuel Consumption: 150.2691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4516.759141014448 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 439.9476 Fuel Consumption: 117.2831\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4552.901737054132 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 443.3913 Fuel Consumption: 118.9888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4572.454123264938 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 445.0435 Fuel Consumption: 122.0191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -3443.485356835054 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 334.4755 Fuel Consumption: 98.7300\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4197.674023355863 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 408.1261 Fuel Consumption: 116.4131\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3889.3758600943015 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 378.1931 Fuel Consumption: 107.4445\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -3710.281518612066 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 360.6444 Fuel Consumption: 103.8375\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4309.90378915404 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 419.6692 Fuel Consumption: 113.2118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -3950.0797619421737 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 384.5157 Fuel Consumption: 104.9225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -3884.7399151107034 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 377.8473 Fuel Consumption: 106.2666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -3697.919689856714 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 359.8212 Fuel Consumption: 99.7075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -2397.1549629288625 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 231.1431 Fuel Consumption: 85.7243\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -3442.062397175935 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 334.9056 Fuel Consumption: 93.0067\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -3213.709397988764 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 311.4280 Fuel Consumption: 99.4289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -2182.9807093687155 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 209.9378 Fuel Consumption: 83.6031\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -3281.6155608398867 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 318.7152 Fuel Consumption: 94.4632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -3553.452417838847 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 346.0876 Fuel Consumption: 92.5766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -2769.299481653893 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 268.5027 Fuel Consumption: 84.2727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -3202.93247532829 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 311.0346 Fuel Consumption: 92.5864\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -3316.391988392979 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 322.7608 Fuel Consumption: 88.7839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -3299.4916886992155 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 320.9061 Fuel Consumption: 90.4310\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -3272.4134073284836 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 318.1240 Fuel Consumption: 91.1739\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -2269.529448969018 Explore P: 0.2347 SOC: 0.9668 Cumulative_SOC_deviation: 219.2803 Fuel Consumption: 76.7263\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -2250.118756817414 Explore P: 0.2286 SOC: 0.9776 Cumulative_SOC_deviation: 217.2190 Fuel Consumption: 77.9286\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -2378.8428990482103 Explore P: 0.2227 SOC: 0.9956 Cumulative_SOC_deviation: 229.8993 Fuel Consumption: 79.8494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -2177.6793330049118 Explore P: 0.2170 SOC: 0.9674 Cumulative_SOC_deviation: 210.0469 Fuel Consumption: 77.2102\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -1823.9639799014917 Explore P: 0.2114 SOC: 0.9654 Cumulative_SOC_deviation: 174.7042 Fuel Consumption: 76.9218\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -2968.3459368424806 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 288.6097 Fuel Consumption: 82.2492\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -1648.7439320844378 Explore P: 0.2006 SOC: 0.9709 Cumulative_SOC_deviation: 157.2252 Fuel Consumption: 76.4917\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -2364.81024107339 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 226.9604 Fuel Consumption: 95.2061\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -1586.0180561699458 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 150.6002 Fuel Consumption: 80.0156\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -2627.885246400774 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 250.9111 Fuel Consumption: 118.7738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -1408.1418691554895 Explore P: 0.1808 SOC: 0.9236 Cumulative_SOC_deviation: 133.5277 Fuel Consumption: 72.8651\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -961.5659603535515 Explore P: 0.1761 SOC: 0.8500 Cumulative_SOC_deviation: 89.4424 Fuel Consumption: 67.1418\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -1220.5160432061887 Explore P: 0.1716 SOC: 0.9517 Cumulative_SOC_deviation: 114.4709 Fuel Consumption: 75.8074\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -900.3792540993404 Explore P: 0.1673 SOC: 0.8061 Cumulative_SOC_deviation: 83.6351 Fuel Consumption: 64.0285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -1470.8938752962115 Explore P: 0.1630 SOC: 0.9329 Cumulative_SOC_deviation: 139.6548 Fuel Consumption: 74.3461\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -1307.6703638645251 Explore P: 0.1589 SOC: 0.8926 Cumulative_SOC_deviation: 123.7088 Fuel Consumption: 70.5827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -1409.8038858620816 Explore P: 0.1548 SOC: 0.9157 Cumulative_SOC_deviation: 133.7364 Fuel Consumption: 72.4399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -813.7137323274032 Explore P: 0.1509 SOC: 0.8179 Cumulative_SOC_deviation: 74.7857 Fuel Consumption: 65.8564\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -1240.7732431734157 Explore P: 0.1471 SOC: 0.9286 Cumulative_SOC_deviation: 116.5596 Fuel Consumption: 75.1769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -962.0263124550978 Explore P: 0.1434 SOC: 0.8672 Cumulative_SOC_deviation: 89.3462 Fuel Consumption: 68.5641\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -927.7888974514684 Explore P: 0.1398 SOC: 0.8559 Cumulative_SOC_deviation: 85.8780 Fuel Consumption: 69.0089\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -973.7147433370441 Explore P: 0.1362 SOC: 0.9188 Cumulative_SOC_deviation: 89.9217 Fuel Consumption: 74.4976\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -527.2197848039355 Explore P: 0.1328 SOC: 0.7010 Cumulative_SOC_deviation: 46.9965 Fuel Consumption: 57.2543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -1146.848182047074 Explore P: 0.1295 SOC: 0.9201 Cumulative_SOC_deviation: 107.3113 Fuel Consumption: 73.7351\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -699.7032553988071 Explore P: 0.1263 SOC: 0.7629 Cumulative_SOC_deviation: 63.8651 Fuel Consumption: 61.0520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -1584.853411216427 Explore P: 0.1231 SOC: 0.9633 Cumulative_SOC_deviation: 150.8279 Fuel Consumption: 76.5748\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -536.2706876440495 Explore P: 0.1200 SOC: 0.7845 Cumulative_SOC_deviation: 47.3445 Fuel Consumption: 62.8261\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -438.33096445538695 Explore P: 0.1171 SOC: 0.6904 Cumulative_SOC_deviation: 38.2641 Fuel Consumption: 55.6903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -374.31679256322616 Explore P: 0.1142 SOC: 0.6867 Cumulative_SOC_deviation: 31.8905 Fuel Consumption: 55.4117\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -739.3602971460613 Explore P: 0.1113 SOC: 0.8147 Cumulative_SOC_deviation: 67.2751 Fuel Consumption: 66.6091\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -696.4216955334698 Explore P: 0.1086 SOC: 0.8190 Cumulative_SOC_deviation: 63.1069 Fuel Consumption: 65.3530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -724.2141641082434 Explore P: 0.1059 SOC: 0.9034 Cumulative_SOC_deviation: 65.3006 Fuel Consumption: 71.2083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -364.19242695611786 Explore P: 0.1033 SOC: 0.6599 Cumulative_SOC_deviation: 31.1146 Fuel Consumption: 53.0462\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -697.2569316442784 Explore P: 0.1008 SOC: 0.8013 Cumulative_SOC_deviation: 63.3468 Fuel Consumption: 63.7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -371.9204843542257 Explore P: 0.0983 SOC: 0.6632 Cumulative_SOC_deviation: 31.8127 Fuel Consumption: 53.7940\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -405.3408538660224 Explore P: 0.0960 SOC: 0.6734 Cumulative_SOC_deviation: 35.0643 Fuel Consumption: 54.6982\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -480.5434038728673 Explore P: 0.0936 SOC: 0.6962 Cumulative_SOC_deviation: 42.4306 Fuel Consumption: 56.2377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -473.2503489973189 Explore P: 0.0914 SOC: 0.7078 Cumulative_SOC_deviation: 41.5722 Fuel Consumption: 57.5280\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -353.26379006508637 Explore P: 0.0892 SOC: 0.6572 Cumulative_SOC_deviation: 29.9504 Fuel Consumption: 53.7597\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -329.3110790849326 Explore P: 0.0870 SOC: 0.6926 Cumulative_SOC_deviation: 27.3469 Fuel Consumption: 55.8418\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -644.2574917525237 Explore P: 0.0849 SOC: 0.8398 Cumulative_SOC_deviation: 57.7639 Fuel Consumption: 66.6189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -356.5314056104336 Explore P: 0.0829 SOC: 0.6419 Cumulative_SOC_deviation: 30.4262 Fuel Consumption: 52.2690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -333.9986640264616 Explore P: 0.0809 SOC: 0.6590 Cumulative_SOC_deviation: 28.1529 Fuel Consumption: 52.4694\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -271.8839413972507 Explore P: 0.0790 SOC: 0.6251 Cumulative_SOC_deviation: 22.0739 Fuel Consumption: 51.1449\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -319.581647267906 Explore P: 0.0771 SOC: 0.6404 Cumulative_SOC_deviation: 26.8212 Fuel Consumption: 51.3697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -442.39332846152956 Explore P: 0.0753 SOC: 0.6335 Cumulative_SOC_deviation: 39.0882 Fuel Consumption: 51.5115\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -367.46736567105125 Explore P: 0.0735 SOC: 0.6331 Cumulative_SOC_deviation: 31.4778 Fuel Consumption: 52.6894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -293.8044168399925 Explore P: 0.0718 SOC: 0.6362 Cumulative_SOC_deviation: 24.3305 Fuel Consumption: 50.4998\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -276.5066379152938 Explore P: 0.0701 SOC: 0.6311 Cumulative_SOC_deviation: 22.6471 Fuel Consumption: 50.0354\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -288.36366593821424 Explore P: 0.0685 SOC: 0.6332 Cumulative_SOC_deviation: 23.6637 Fuel Consumption: 51.7265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -279.38627375923386 Explore P: 0.0669 SOC: 0.6307 Cumulative_SOC_deviation: 22.7366 Fuel Consumption: 52.0198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -236.23058591193004 Explore P: 0.0654 SOC: 0.6333 Cumulative_SOC_deviation: 18.5599 Fuel Consumption: 50.6317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -255.96804724074272 Explore P: 0.0639 SOC: 0.6703 Cumulative_SOC_deviation: 20.2316 Fuel Consumption: 53.6522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -298.3090585250076 Explore P: 0.0624 SOC: 0.6921 Cumulative_SOC_deviation: 24.2629 Fuel Consumption: 55.6805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -1714.6075274050017 Explore P: 0.0610 SOC: 0.9378 Cumulative_SOC_deviation: 164.1390 Fuel Consumption: 73.2170\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -326.3514155106329 Explore P: 0.0596 SOC: 0.6225 Cumulative_SOC_deviation: 27.5329 Fuel Consumption: 51.0227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -267.11366204546823 Explore P: 0.0583 SOC: 0.6398 Cumulative_SOC_deviation: 21.5201 Fuel Consumption: 51.9123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -243.99492975175434 Explore P: 0.0570 SOC: 0.6292 Cumulative_SOC_deviation: 19.3554 Fuel Consumption: 50.4411\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -228.14384903768317 Explore P: 0.0557 SOC: 0.6219 Cumulative_SOC_deviation: 17.8333 Fuel Consumption: 49.8106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -299.8028352280999 Explore P: 0.0545 SOC: 0.6215 Cumulative_SOC_deviation: 24.9958 Fuel Consumption: 49.8448\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -235.61467240688268 Explore P: 0.0533 SOC: 0.6325 Cumulative_SOC_deviation: 18.3610 Fuel Consumption: 52.0051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -264.36126954097995 Explore P: 0.0521 SOC: 0.6316 Cumulative_SOC_deviation: 21.3446 Fuel Consumption: 50.9152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -242.75811485905638 Explore P: 0.0510 SOC: 0.6203 Cumulative_SOC_deviation: 19.2532 Fuel Consumption: 50.2261\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -219.99271483235785 Explore P: 0.0498 SOC: 0.6157 Cumulative_SOC_deviation: 17.0417 Fuel Consumption: 49.5760\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -335.5952631487126 Explore P: 0.0488 SOC: 0.7808 Cumulative_SOC_deviation: 27.3126 Fuel Consumption: 62.4693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -249.8253311526008 Explore P: 0.0477 SOC: 0.6332 Cumulative_SOC_deviation: 19.8343 Fuel Consumption: 51.4822\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -236.93036913926434 Explore P: 0.0467 SOC: 0.6194 Cumulative_SOC_deviation: 18.7174 Fuel Consumption: 49.7569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -299.8956646113342 Explore P: 0.0457 SOC: 0.6068 Cumulative_SOC_deviation: 25.1351 Fuel Consumption: 48.5447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -453.4388356017494 Explore P: 0.0447 SOC: 0.7857 Cumulative_SOC_deviation: 38.9635 Fuel Consumption: 63.8036\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -242.69535795298535 Explore P: 0.0438 SOC: 0.6251 Cumulative_SOC_deviation: 19.1370 Fuel Consumption: 51.3258\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -227.9902366511503 Explore P: 0.0429 SOC: 0.6171 Cumulative_SOC_deviation: 17.7231 Fuel Consumption: 50.7588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -1409.5368166407916 Explore P: 0.0420 SOC: 1.0000 Cumulative_SOC_deviation: 131.8500 Fuel Consumption: 91.0370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -230.03296602484932 Explore P: 0.0411 SOC: 0.6193 Cumulative_SOC_deviation: 17.9098 Fuel Consumption: 50.9347\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -273.7099210968727 Explore P: 0.0403 SOC: 0.6269 Cumulative_SOC_deviation: 22.1602 Fuel Consumption: 52.1078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -551.5834503483464 Explore P: 0.0395 SOC: 0.9298 Cumulative_SOC_deviation: 47.6788 Fuel Consumption: 74.7957\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -329.1384769814634 Explore P: 0.0387 SOC: 0.6097 Cumulative_SOC_deviation: 27.9323 Fuel Consumption: 49.8155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -299.463849097199 Explore P: 0.0379 SOC: 0.6063 Cumulative_SOC_deviation: 25.0518 Fuel Consumption: 48.9455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -269.6675565878096 Explore P: 0.0371 SOC: 0.6143 Cumulative_SOC_deviation: 21.9266 Fuel Consumption: 50.4020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -296.9621523313319 Explore P: 0.0364 SOC: 0.6249 Cumulative_SOC_deviation: 24.6145 Fuel Consumption: 50.8174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -273.1303818089388 Explore P: 0.0357 SOC: 0.6105 Cumulative_SOC_deviation: 22.3178 Fuel Consumption: 49.9524\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -305.76836229628447 Explore P: 0.0350 SOC: 0.6071 Cumulative_SOC_deviation: 25.5420 Fuel Consumption: 50.3482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -282.3312363077049 Explore P: 0.0343 SOC: 0.6048 Cumulative_SOC_deviation: 23.2330 Fuel Consumption: 50.0012\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -282.951724850679 Explore P: 0.0336 SOC: 0.6102 Cumulative_SOC_deviation: 23.2525 Fuel Consumption: 50.4264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -395.92145845879026 Explore P: 0.0330 SOC: 0.6139 Cumulative_SOC_deviation: 34.5402 Fuel Consumption: 50.5193\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -315.16784649962705 Explore P: 0.0324 SOC: 0.6083 Cumulative_SOC_deviation: 26.6374 Fuel Consumption: 48.7940\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -315.14856171116406 Explore P: 0.0318 SOC: 0.6166 Cumulative_SOC_deviation: 26.5641 Fuel Consumption: 49.5076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -294.78137227212596 Explore P: 0.0312 SOC: 0.5987 Cumulative_SOC_deviation: 24.6481 Fuel Consumption: 48.3004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -316.4991723154521 Explore P: 0.0306 SOC: 0.6064 Cumulative_SOC_deviation: 26.8042 Fuel Consumption: 48.4568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -296.7532260967334 Explore P: 0.0301 SOC: 0.5992 Cumulative_SOC_deviation: 24.7910 Fuel Consumption: 48.8429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -333.3150180593383 Explore P: 0.0295 SOC: 0.6117 Cumulative_SOC_deviation: 28.3436 Fuel Consumption: 49.8790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -258.03475384592963 Explore P: 0.0290 SOC: 0.6166 Cumulative_SOC_deviation: 20.8180 Fuel Consumption: 49.8546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -253.37416709229691 Explore P: 0.0285 SOC: 0.6121 Cumulative_SOC_deviation: 20.2474 Fuel Consumption: 50.9005\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -317.39184616657536 Explore P: 0.0280 SOC: 0.6042 Cumulative_SOC_deviation: 26.8534 Fuel Consumption: 48.8575\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -284.68800072965286 Explore P: 0.0275 SOC: 0.6046 Cumulative_SOC_deviation: 23.5889 Fuel Consumption: 48.7989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -300.5869326192869 Explore P: 0.0270 SOC: 0.6060 Cumulative_SOC_deviation: 25.1998 Fuel Consumption: 48.5887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -255.05245802973874 Explore P: 0.0265 SOC: 0.6052 Cumulative_SOC_deviation: 20.6713 Fuel Consumption: 48.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -360.91370938261565 Explore P: 0.0261 SOC: 0.6028 Cumulative_SOC_deviation: 31.2853 Fuel Consumption: 48.0609\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -259.70660833371784 Explore P: 0.0257 SOC: 0.6071 Cumulative_SOC_deviation: 21.0013 Fuel Consumption: 49.6933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -341.02257122068914 Explore P: 0.0252 SOC: 0.5947 Cumulative_SOC_deviation: 29.2087 Fuel Consumption: 48.9357\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -257.5062393789194 Explore P: 0.0248 SOC: 0.6069 Cumulative_SOC_deviation: 20.8287 Fuel Consumption: 49.2192\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -289.55948627197756 Explore P: 0.0244 SOC: 0.6042 Cumulative_SOC_deviation: 24.0521 Fuel Consumption: 49.0384\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -303.67388078437006 Explore P: 0.0240 SOC: 0.6016 Cumulative_SOC_deviation: 25.4831 Fuel Consumption: 48.8429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -356.54990345116346 Explore P: 0.0237 SOC: 0.5940 Cumulative_SOC_deviation: 30.7917 Fuel Consumption: 48.6327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -356.7899805098541 Explore P: 0.0233 SOC: 0.6113 Cumulative_SOC_deviation: 30.6466 Fuel Consumption: 50.3238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -347.45077282284035 Explore P: 0.0229 SOC: 0.5977 Cumulative_SOC_deviation: 29.8911 Fuel Consumption: 48.5399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -340.9776492333887 Explore P: 0.0226 SOC: 0.5999 Cumulative_SOC_deviation: 29.1587 Fuel Consumption: 49.3903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -287.0419314803214 Explore P: 0.0222 SOC: 0.5972 Cumulative_SOC_deviation: 23.7485 Fuel Consumption: 49.5565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -279.68748003043675 Explore P: 0.0219 SOC: 0.6012 Cumulative_SOC_deviation: 22.9549 Fuel Consumption: 50.1381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -290.66133052048383 Explore P: 0.0216 SOC: 0.6041 Cumulative_SOC_deviation: 24.0846 Fuel Consumption: 49.8155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -330.3556719613194 Explore P: 0.0213 SOC: 0.5979 Cumulative_SOC_deviation: 28.2207 Fuel Consumption: 48.1489\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -328.2032600859696 Explore P: 0.0210 SOC: 0.6008 Cumulative_SOC_deviation: 28.0372 Fuel Consumption: 47.8312\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -369.31540096488334 Explore P: 0.0207 SOC: 0.6091 Cumulative_SOC_deviation: 32.0116 Fuel Consumption: 49.1997\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -261.52293438500095 Explore P: 0.0204 SOC: 0.6028 Cumulative_SOC_deviation: 21.1839 Fuel Consumption: 49.6835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -246.85832086486585 Explore P: 0.0201 SOC: 0.5991 Cumulative_SOC_deviation: 19.8661 Fuel Consumption: 48.1977\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -269.7131952315187 Explore P: 0.0198 SOC: 0.6050 Cumulative_SOC_deviation: 22.1379 Fuel Consumption: 48.3346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -265.338251782775 Explore P: 0.0196 SOC: 0.6080 Cumulative_SOC_deviation: 21.6598 Fuel Consumption: 48.7402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -241.07080275598778 Explore P: 0.0193 SOC: 0.6020 Cumulative_SOC_deviation: 19.1910 Fuel Consumption: 49.1606\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -217.87653115818924 Explore P: 0.0190 SOC: 0.6039 Cumulative_SOC_deviation: 16.8227 Fuel Consumption: 49.6493\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -194.42075705357703 Explore P: 0.0188 SOC: 0.6123 Cumulative_SOC_deviation: 14.4727 Fuel Consumption: 49.6933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -183.3785465354547 Explore P: 0.0186 SOC: 0.6080 Cumulative_SOC_deviation: 13.5313 Fuel Consumption: 48.0658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -254.68590940387264 Explore P: 0.0183 SOC: 0.6181 Cumulative_SOC_deviation: 20.5261 Fuel Consumption: 49.4245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -217.18831786700997 Explore P: 0.0181 SOC: 0.6155 Cumulative_SOC_deviation: 16.7143 Fuel Consumption: 50.0452\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -194.36462048616576 Explore P: 0.0179 SOC: 0.6087 Cumulative_SOC_deviation: 14.4398 Fuel Consumption: 49.9670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -176.2314097126036 Explore P: 0.0177 SOC: 0.6109 Cumulative_SOC_deviation: 12.6685 Fuel Consumption: 49.5467\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -171.50656984772016 Explore P: 0.0175 SOC: 0.6152 Cumulative_SOC_deviation: 12.2625 Fuel Consumption: 48.8820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -206.45630464989426 Explore P: 0.0173 SOC: 0.6530 Cumulative_SOC_deviation: 15.4955 Fuel Consumption: 51.5017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -182.14470495932184 Explore P: 0.0171 SOC: 0.6120 Cumulative_SOC_deviation: 13.3214 Fuel Consumption: 48.9309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -471.5095662000517 Explore P: 0.0169 SOC: 0.6737 Cumulative_SOC_deviation: 41.8894 Fuel Consumption: 52.6161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -184.53833093901787 Explore P: 0.0167 SOC: 0.6095 Cumulative_SOC_deviation: 13.5593 Fuel Consumption: 48.9455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -357.72244305432673 Explore P: 0.0165 SOC: 0.8165 Cumulative_SOC_deviation: 29.2995 Fuel Consumption: 64.7274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -192.18578692962814 Explore P: 0.0163 SOC: 0.6081 Cumulative_SOC_deviation: 14.3064 Fuel Consumption: 49.1215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -232.43438804381205 Explore P: 0.0162 SOC: 0.7165 Cumulative_SOC_deviation: 17.4291 Fuel Consumption: 58.1439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -224.34489934853318 Explore P: 0.0160 SOC: 0.6134 Cumulative_SOC_deviation: 17.4153 Fuel Consumption: 50.1918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -378.9869938798374 Explore P: 0.0158 SOC: 0.8876 Cumulative_SOC_deviation: 30.8165 Fuel Consumption: 70.8221\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -216.95016757015932 Explore P: 0.0157 SOC: 0.6103 Cumulative_SOC_deviation: 16.8361 Fuel Consumption: 48.5887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -202.64259935389083 Explore P: 0.0155 SOC: 0.6108 Cumulative_SOC_deviation: 15.3594 Fuel Consumption: 49.0482\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -188.15968954792334 Explore P: 0.0154 SOC: 0.6075 Cumulative_SOC_deviation: 13.8662 Fuel Consumption: 49.4978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -174.54048607914044 Explore P: 0.0152 SOC: 0.6093 Cumulative_SOC_deviation: 12.6333 Fuel Consumption: 48.2075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -174.68388315797552 Explore P: 0.0151 SOC: 0.6148 Cumulative_SOC_deviation: 12.5782 Fuel Consumption: 48.9015\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -259.0656733279492 Explore P: 0.0149 SOC: 0.6309 Cumulative_SOC_deviation: 20.8253 Fuel Consumption: 50.8126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -347.84306446317487 Explore P: 0.0148 SOC: 0.8017 Cumulative_SOC_deviation: 28.4323 Fuel Consumption: 63.5202\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -201.52562062259264 Explore P: 0.0147 SOC: 0.6057 Cumulative_SOC_deviation: 15.1989 Fuel Consumption: 49.5369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -200.67795042105897 Explore P: 0.0146 SOC: 0.6114 Cumulative_SOC_deviation: 15.1591 Fuel Consumption: 49.0873\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -1633.525269389176 Explore P: 0.0144 SOC: 1.0000 Cumulative_SOC_deviation: 154.2948 Fuel Consumption: 90.5776\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -206.80157337079308 Explore P: 0.0143 SOC: 0.6110 Cumulative_SOC_deviation: 15.7436 Fuel Consumption: 49.3658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -191.91838373302738 Explore P: 0.0142 SOC: 0.6069 Cumulative_SOC_deviation: 14.3354 Fuel Consumption: 48.5643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -180.73212787082366 Explore P: 0.0141 SOC: 0.6019 Cumulative_SOC_deviation: 13.2378 Fuel Consumption: 48.3541\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_2_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
