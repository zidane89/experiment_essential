{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0 \n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self.N += 1 \n",
    "        power = sample[0][0]\n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2\n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N)\n",
    "        \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    next_power_states = np.array([0 if val[3] is None else (val[3][0] - memory.power_mean) / memory.power_std  \n",
    "                        for val in batch])\n",
    "    \n",
    "    states[:, 0] = (states[:, 0] - memory.power_mean) / memory.power_std \n",
    "    next_states[:, 0] = next_power_states\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5094.42117258183 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 495.2549 Fuel Consumption: 141.8723\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5028.142298259511 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 488.3352 Fuel Consumption: 144.7902\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4965.458724028476 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 482.9207 Fuel Consumption: 136.2516\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4949.645037025506 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 481.1854 Fuel Consumption: 137.7912\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5042.2413864084265 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 490.0951 Fuel Consumption: 141.2907\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4971.398277318659 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 483.5440 Fuel Consumption: 135.9584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5007.633195325981 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 487.1499 Fuel Consumption: 136.1343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4986.6640002837885 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 485.2343 Fuel Consumption: 134.3211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4886.014718874387 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 474.5164 Fuel Consumption: 140.8508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4949.11802005704 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 480.7334 Fuel Consumption: 141.7843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4890.219728818699 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 475.2521 Fuel Consumption: 137.6983\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4955.303300076205 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 482.2605 Fuel Consumption: 132.6984\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4787.989387767976 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 465.3336 Fuel Consumption: 134.6534\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4816.608855531079 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 468.4746 Fuel Consumption: 131.8626\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4830.371280766776 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 469.3470 Fuel Consumption: 136.9017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4895.322040873095 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 475.4041 Fuel Consumption: 141.2809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4712.328238123588 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 458.3276 Fuel Consumption: 129.0523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4627.163016636688 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 450.4567 Fuel Consumption: 122.5958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4499.388977015718 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 437.7595 Fuel Consumption: 121.7943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4341.524518500999 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 422.1475 Fuel Consumption: 120.0494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4426.20316059668 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 430.7625 Fuel Consumption: 118.5783\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4599.517058318165 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 447.7200 Fuel Consumption: 122.3173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4353.257394621338 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 423.1106 Fuel Consumption: 122.1511\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4408.61329394624 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 427.9278 Fuel Consumption: 129.3358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4338.7170803962335 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 421.4425 Fuel Consumption: 124.2918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4332.496714966564 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 420.7491 Fuel Consumption: 125.0054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4154.6813999239075 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 403.2076 Fuel Consumption: 122.6056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4191.067754806198 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 407.8203 Fuel Consumption: 112.8648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4171.894483816726 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 406.3238 Fuel Consumption: 108.6566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -3761.2910827533406 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 364.9057 Fuel Consumption: 112.2343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -3489.3348335008577 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 338.6587 Fuel Consumption: 102.7476\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4004.2386701731557 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 389.5856 Fuel Consumption: 108.3829\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -3868.7080901785534 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 374.9578 Fuel Consumption: 119.1306\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -3807.6708051372525 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 370.2294 Fuel Consumption: 105.3770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3869.9507728220383 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 376.2057 Fuel Consumption: 107.8941\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -3341.801471410914 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 324.5999 Fuel Consumption: 95.8024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -3530.4232548362506 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 343.4284 Fuel Consumption: 96.1396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -3071.90765553548 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 298.2909 Fuel Consumption: 88.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -3378.3400316164066 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 328.3896 Fuel Consumption: 94.4436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -2696.0910015932427 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 261.2268 Fuel Consumption: 83.8230\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -2718.998913982208 Explore P: 0.3140 SOC: 0.9992 Cumulative_SOC_deviation: 263.8167 Fuel Consumption: 80.8318\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -2825.8775612117074 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 274.4513 Fuel Consumption: 81.3646\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -2700.2626837889193 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 261.3659 Fuel Consumption: 86.6040\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -1878.4037938657211 Explore P: 0.2899 SOC: 0.9220 Cumulative_SOC_deviation: 180.2127 Fuel Consumption: 76.2766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -3014.0281714741814 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 292.5684 Fuel Consumption: 88.3440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -1378.5172826622613 Explore P: 0.2750 SOC: 0.8972 Cumulative_SOC_deviation: 130.2896 Fuel Consumption: 75.6217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -1338.695339834469 Explore P: 0.2678 SOC: 0.9469 Cumulative_SOC_deviation: 125.9569 Fuel Consumption: 79.1261\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -2238.1283890032682 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 214.5918 Fuel Consumption: 92.2100\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -1335.3781107815653 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 125.0367 Fuel Consumption: 85.0107\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -1849.573837014032 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 175.8136 Fuel Consumption: 91.4378\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1455.610746685271 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 137.0018 Fuel Consumption: 85.5923\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -1181.980500093777 Explore P: 0.2347 SOC: 0.9151 Cumulative_SOC_deviation: 110.6965 Fuel Consumption: 75.0157\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -1462.1379152188526 Explore P: 0.2286 SOC: 0.9141 Cumulative_SOC_deviation: 138.7352 Fuel Consumption: 74.7859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -1564.4093955561705 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 147.7590 Fuel Consumption: 86.8191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -1064.1987893015787 Explore P: 0.2170 SOC: 0.8427 Cumulative_SOC_deviation: 99.4330 Fuel Consumption: 69.8691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -1485.617560224263 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 140.3823 Fuel Consumption: 81.7947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -1385.1365417455163 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 130.0248 Fuel Consumption: 84.8885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -1160.2067312632594 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 107.4277 Fuel Consumption: 85.9295\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -830.2154162460369 Explore P: 0.1954 SOC: 0.9355 Cumulative_SOC_deviation: 75.3655 Fuel Consumption: 76.5601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -734.472461925985 Explore P: 0.1904 SOC: 0.8160 Cumulative_SOC_deviation: 66.6842 Fuel Consumption: 67.6306\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -1590.3641731801147 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 150.3315 Fuel Consumption: 87.0488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -621.1656060213833 Explore P: 0.1808 SOC: 0.7080 Cumulative_SOC_deviation: 56.2323 Fuel Consumption: 58.8428\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -1161.136019886062 Explore P: 0.1761 SOC: 0.9415 Cumulative_SOC_deviation: 108.6389 Fuel Consumption: 74.7468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -709.8536210453641 Explore P: 0.1716 SOC: 0.7243 Cumulative_SOC_deviation: 65.0263 Fuel Consumption: 59.5906\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -2245.068609982959 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 215.3929 Fuel Consumption: 91.1396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -538.3967637629037 Explore P: 0.1630 SOC: 0.6892 Cumulative_SOC_deviation: 48.1494 Fuel Consumption: 56.9024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -718.7201077130376 Explore P: 0.1589 SOC: 0.7198 Cumulative_SOC_deviation: 66.0078 Fuel Consumption: 58.6424\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -544.344329044403 Explore P: 0.1548 SOC: 0.7027 Cumulative_SOC_deviation: 48.7417 Fuel Consumption: 56.9269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -2473.51161698558 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 238.3007 Fuel Consumption: 90.5043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -512.2579656434931 Explore P: 0.1471 SOC: 0.6856 Cumulative_SOC_deviation: 45.6177 Fuel Consumption: 56.0813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -658.3603580790999 Explore P: 0.1434 SOC: 0.7081 Cumulative_SOC_deviation: 60.0353 Fuel Consumption: 58.0070\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -453.38059265628186 Explore P: 0.1398 SOC: 0.6491 Cumulative_SOC_deviation: 40.0437 Fuel Consumption: 52.9435\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -419.63287372558824 Explore P: 0.1362 SOC: 0.6540 Cumulative_SOC_deviation: 36.6049 Fuel Consumption: 53.5838\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -414.8343128805918 Explore P: 0.1328 SOC: 0.6507 Cumulative_SOC_deviation: 36.1998 Fuel Consumption: 52.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -502.25534865873834 Explore P: 0.1295 SOC: 0.6666 Cumulative_SOC_deviation: 44.7596 Fuel Consumption: 54.6591\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -615.9344255014425 Explore P: 0.1263 SOC: 0.6800 Cumulative_SOC_deviation: 56.0537 Fuel Consumption: 55.3971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -1775.0556398211156 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 169.0925 Fuel Consumption: 84.1309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -607.2343162582341 Explore P: 0.1200 SOC: 0.7272 Cumulative_SOC_deviation: 54.8743 Fuel Consumption: 58.4909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -462.9181379937955 Explore P: 0.1171 SOC: 0.6524 Cumulative_SOC_deviation: 41.0810 Fuel Consumption: 52.1078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -559.0726356008039 Explore P: 0.1142 SOC: 0.7078 Cumulative_SOC_deviation: 50.2625 Fuel Consumption: 56.4479\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -364.96377388044414 Explore P: 0.1113 SOC: 0.6485 Cumulative_SOC_deviation: 31.3447 Fuel Consumption: 51.5164\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -516.0774189900683 Explore P: 0.1086 SOC: 0.6856 Cumulative_SOC_deviation: 46.1506 Fuel Consumption: 54.5711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -375.23399215533254 Explore P: 0.1059 SOC: 0.6460 Cumulative_SOC_deviation: 32.3346 Fuel Consumption: 51.8878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -324.4121287316669 Explore P: 0.1033 SOC: 0.6452 Cumulative_SOC_deviation: 27.2989 Fuel Consumption: 51.4235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -406.66170036711884 Explore P: 0.1008 SOC: 0.6435 Cumulative_SOC_deviation: 35.5399 Fuel Consumption: 51.2622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -345.737164693684 Explore P: 0.0983 SOC: 0.6422 Cumulative_SOC_deviation: 29.4177 Fuel Consumption: 51.5604\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -330.57058533843525 Explore P: 0.0960 SOC: 0.6423 Cumulative_SOC_deviation: 27.9543 Fuel Consumption: 51.0276\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -590.5346063772503 Explore P: 0.0936 SOC: 0.9491 Cumulative_SOC_deviation: 51.4629 Fuel Consumption: 75.9052\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -435.9450890655883 Explore P: 0.0914 SOC: 0.6367 Cumulative_SOC_deviation: 38.5152 Fuel Consumption: 50.7930\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -335.2124074703514 Explore P: 0.0892 SOC: 0.6345 Cumulative_SOC_deviation: 28.4942 Fuel Consumption: 50.2700\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -1492.3614932507892 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 139.5235 Fuel Consumption: 97.1269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -311.75609196498993 Explore P: 0.0849 SOC: 0.6549 Cumulative_SOC_deviation: 25.9834 Fuel Consumption: 51.9220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -340.5622538071236 Explore P: 0.0829 SOC: 0.6377 Cumulative_SOC_deviation: 29.0434 Fuel Consumption: 50.1283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -393.6005617371213 Explore P: 0.0809 SOC: 0.6360 Cumulative_SOC_deviation: 34.2500 Fuel Consumption: 51.1009\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -348.5263467290538 Explore P: 0.0790 SOC: 0.6440 Cumulative_SOC_deviation: 29.6463 Fuel Consumption: 52.0638\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -275.3860596273912 Explore P: 0.0771 SOC: 0.6364 Cumulative_SOC_deviation: 22.4725 Fuel Consumption: 50.6610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -417.0586149487831 Explore P: 0.0753 SOC: 0.6404 Cumulative_SOC_deviation: 36.5640 Fuel Consumption: 51.4186\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -292.1915630079612 Explore P: 0.0735 SOC: 0.6447 Cumulative_SOC_deviation: 24.1086 Fuel Consumption: 51.1058\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -290.82958123154896 Explore P: 0.0718 SOC: 0.6216 Cumulative_SOC_deviation: 24.0750 Fuel Consumption: 50.0794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -380.70674541744836 Explore P: 0.0701 SOC: 0.6115 Cumulative_SOC_deviation: 33.2289 Fuel Consumption: 48.4177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -323.68504183612276 Explore P: 0.0685 SOC: 0.6385 Cumulative_SOC_deviation: 27.3747 Fuel Consumption: 49.9377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -315.38036535004824 Explore P: 0.0669 SOC: 0.6312 Cumulative_SOC_deviation: 26.4842 Fuel Consumption: 50.5389\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -297.0399176484976 Explore P: 0.0654 SOC: 0.6485 Cumulative_SOC_deviation: 24.4986 Fuel Consumption: 52.0540\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -252.78994784805758 Explore P: 0.0639 SOC: 0.6366 Cumulative_SOC_deviation: 20.2417 Fuel Consumption: 50.3727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -244.53847898003897 Explore P: 0.0624 SOC: 0.6326 Cumulative_SOC_deviation: 19.5481 Fuel Consumption: 49.0579\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -293.3784235333507 Explore P: 0.0610 SOC: 0.6238 Cumulative_SOC_deviation: 24.2859 Fuel Consumption: 50.5193\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -408.4421173068348 Explore P: 0.0596 SOC: 0.8259 Cumulative_SOC_deviation: 34.2459 Fuel Consumption: 65.9835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -297.9767479982821 Explore P: 0.0583 SOC: 0.6679 Cumulative_SOC_deviation: 24.6089 Fuel Consumption: 51.8878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -227.85883213677806 Explore P: 0.0570 SOC: 0.6282 Cumulative_SOC_deviation: 17.7906 Fuel Consumption: 49.9524\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -203.67295993814622 Explore P: 0.0557 SOC: 0.6233 Cumulative_SOC_deviation: 15.4899 Fuel Consumption: 48.7745\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -170.53842544600045 Explore P: 0.0545 SOC: 0.6081 Cumulative_SOC_deviation: 12.2737 Fuel Consumption: 47.8018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -201.48460218532725 Explore P: 0.0533 SOC: 0.6167 Cumulative_SOC_deviation: 15.2368 Fuel Consumption: 49.1166\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -185.35795608316303 Explore P: 0.0521 SOC: 0.6211 Cumulative_SOC_deviation: 13.7048 Fuel Consumption: 48.3101\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -213.42957220742218 Explore P: 0.0510 SOC: 0.6217 Cumulative_SOC_deviation: 16.4552 Fuel Consumption: 48.8771\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -215.55364004872277 Explore P: 0.0498 SOC: 0.6174 Cumulative_SOC_deviation: 16.6085 Fuel Consumption: 49.4685\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -177.7877932823845 Explore P: 0.0488 SOC: 0.6123 Cumulative_SOC_deviation: 12.9238 Fuel Consumption: 48.5496\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -187.82213454622377 Explore P: 0.0477 SOC: 0.6075 Cumulative_SOC_deviation: 14.0885 Fuel Consumption: 46.9367\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -189.7774636789071 Explore P: 0.0467 SOC: 0.6145 Cumulative_SOC_deviation: 14.1208 Fuel Consumption: 48.5692\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -177.6342245371355 Explore P: 0.0457 SOC: 0.6142 Cumulative_SOC_deviation: 12.9510 Fuel Consumption: 48.1244\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -157.50853424987244 Explore P: 0.0447 SOC: 0.6149 Cumulative_SOC_deviation: 10.8793 Fuel Consumption: 48.7158\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -182.74273320671418 Explore P: 0.0438 SOC: 0.6204 Cumulative_SOC_deviation: 13.3602 Fuel Consumption: 49.1410\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -185.14467702446535 Explore P: 0.0429 SOC: 0.6184 Cumulative_SOC_deviation: 13.5603 Fuel Consumption: 49.5418\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -208.70703921427113 Explore P: 0.0420 SOC: 0.6276 Cumulative_SOC_deviation: 15.8183 Fuel Consumption: 50.5242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -183.52423366121704 Explore P: 0.0411 SOC: 0.6162 Cumulative_SOC_deviation: 13.4403 Fuel Consumption: 49.1215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -171.14187883813543 Explore P: 0.0403 SOC: 0.6081 Cumulative_SOC_deviation: 12.2978 Fuel Consumption: 48.1635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -198.91877465018547 Explore P: 0.0395 SOC: 0.6142 Cumulative_SOC_deviation: 15.1058 Fuel Consumption: 47.8605\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -169.16791398082484 Explore P: 0.0387 SOC: 0.6091 Cumulative_SOC_deviation: 12.1547 Fuel Consumption: 47.6210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -161.72747480926066 Explore P: 0.0379 SOC: 0.6107 Cumulative_SOC_deviation: 11.3838 Fuel Consumption: 47.8898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -162.92376037924745 Explore P: 0.0371 SOC: 0.6106 Cumulative_SOC_deviation: 11.4545 Fuel Consumption: 48.3786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -178.03496234174156 Explore P: 0.0364 SOC: 0.6132 Cumulative_SOC_deviation: 12.8527 Fuel Consumption: 49.5076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -160.3783073902057 Explore P: 0.0357 SOC: 0.6078 Cumulative_SOC_deviation: 11.1873 Fuel Consumption: 48.5056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -147.0376300274281 Explore P: 0.0350 SOC: 0.6227 Cumulative_SOC_deviation: 9.7457 Fuel Consumption: 49.5809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -238.76538621284826 Explore P: 0.0343 SOC: 0.6075 Cumulative_SOC_deviation: 18.9272 Fuel Consumption: 49.4929\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -221.8652741328727 Explore P: 0.0336 SOC: 0.6776 Cumulative_SOC_deviation: 16.8369 Fuel Consumption: 53.4958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -167.82207082255465 Explore P: 0.0330 SOC: 0.6075 Cumulative_SOC_deviation: 11.9668 Fuel Consumption: 48.1537\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -170.8438270860071 Explore P: 0.0324 SOC: 0.6076 Cumulative_SOC_deviation: 12.2759 Fuel Consumption: 48.0853\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -147.19367330509894 Explore P: 0.0318 SOC: 0.6103 Cumulative_SOC_deviation: 9.9494 Fuel Consumption: 47.6992\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -154.99028116191968 Explore P: 0.0312 SOC: 0.6113 Cumulative_SOC_deviation: 10.6015 Fuel Consumption: 48.9748\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -162.43938522556945 Explore P: 0.0306 SOC: 0.6057 Cumulative_SOC_deviation: 11.4730 Fuel Consumption: 47.7090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -150.45394623658743 Explore P: 0.0301 SOC: 0.6050 Cumulative_SOC_deviation: 10.2036 Fuel Consumption: 48.4177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -158.7211734026693 Explore P: 0.0295 SOC: 0.6060 Cumulative_SOC_deviation: 11.1071 Fuel Consumption: 47.6503\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -180.3601723355975 Explore P: 0.0290 SOC: 0.6092 Cumulative_SOC_deviation: 13.2246 Fuel Consumption: 48.1146\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -148.0725300167728 Explore P: 0.0285 SOC: 0.6118 Cumulative_SOC_deviation: 9.9166 Fuel Consumption: 48.9064\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -141.00399092250885 Explore P: 0.0280 SOC: 0.6059 Cumulative_SOC_deviation: 9.3144 Fuel Consumption: 47.8605\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -159.2045315305577 Explore P: 0.0275 SOC: 0.6100 Cumulative_SOC_deviation: 11.1041 Fuel Consumption: 48.1635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -149.11974398064484 Explore P: 0.0270 SOC: 0.6151 Cumulative_SOC_deviation: 10.1152 Fuel Consumption: 47.9680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -182.4772809316748 Explore P: 0.0265 SOC: 0.6728 Cumulative_SOC_deviation: 12.9964 Fuel Consumption: 52.5134\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -1800.4749279797984 Explore P: 0.0261 SOC: 1.0000 Cumulative_SOC_deviation: 171.7014 Fuel Consumption: 83.4613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -373.0891307624323 Explore P: 0.0257 SOC: 0.6307 Cumulative_SOC_deviation: 32.1666 Fuel Consumption: 51.4235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -2423.2774150554246 Explore P: 0.0252 SOC: 1.0000 Cumulative_SOC_deviation: 232.4474 Fuel Consumption: 98.8033\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -228.1151767629538 Explore P: 0.0248 SOC: 0.6097 Cumulative_SOC_deviation: 17.7865 Fuel Consumption: 50.2505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -128.63993536900736 Explore P: 0.0244 SOC: 0.6073 Cumulative_SOC_deviation: 8.0853 Fuel Consumption: 47.7872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -141.30706903982255 Explore P: 0.0240 SOC: 0.6075 Cumulative_SOC_deviation: 9.3618 Fuel Consumption: 47.6894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -134.2994809035079 Explore P: 0.0237 SOC: 0.6088 Cumulative_SOC_deviation: 8.7250 Fuel Consumption: 47.0492\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -122.0608041285979 Explore P: 0.0233 SOC: 0.6121 Cumulative_SOC_deviation: 7.3467 Fuel Consumption: 48.5936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -136.13803207765616 Explore P: 0.0229 SOC: 0.6141 Cumulative_SOC_deviation: 8.7955 Fuel Consumption: 48.1831\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -120.76867856114333 Explore P: 0.0226 SOC: 0.6057 Cumulative_SOC_deviation: 7.3069 Fuel Consumption: 47.6992\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -116.70253457870966 Explore P: 0.0222 SOC: 0.6065 Cumulative_SOC_deviation: 6.9389 Fuel Consumption: 47.3131\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -112.34589048984203 Explore P: 0.0219 SOC: 0.6006 Cumulative_SOC_deviation: 6.6260 Fuel Consumption: 46.0863\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -114.68056902830577 Explore P: 0.0216 SOC: 0.6077 Cumulative_SOC_deviation: 6.7441 Fuel Consumption: 47.2398\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -116.5024012106482 Explore P: 0.0213 SOC: 0.6076 Cumulative_SOC_deviation: 6.8564 Fuel Consumption: 47.9387\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -119.16074794361245 Explore P: 0.0210 SOC: 0.6040 Cumulative_SOC_deviation: 7.1457 Fuel Consumption: 47.7041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -113.60866146663689 Explore P: 0.0207 SOC: 0.6056 Cumulative_SOC_deviation: 6.6863 Fuel Consumption: 46.7461\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -112.55992580313776 Explore P: 0.0204 SOC: 0.6045 Cumulative_SOC_deviation: 6.5868 Fuel Consumption: 46.6924\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -110.84114039261546 Explore P: 0.0201 SOC: 0.6041 Cumulative_SOC_deviation: 6.3025 Fuel Consumption: 47.8165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -111.47150258960532 Explore P: 0.0198 SOC: 0.6046 Cumulative_SOC_deviation: 6.4144 Fuel Consumption: 47.3277\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -108.96544197870418 Explore P: 0.0196 SOC: 0.6034 Cumulative_SOC_deviation: 6.1081 Fuel Consumption: 47.8849\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -99.44853598139775 Explore P: 0.0193 SOC: 0.6043 Cumulative_SOC_deviation: 5.1407 Fuel Consumption: 48.0413\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -117.31572920548714 Explore P: 0.0190 SOC: 0.6075 Cumulative_SOC_deviation: 6.9015 Fuel Consumption: 48.3004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -110.17764810503014 Explore P: 0.0188 SOC: 0.6135 Cumulative_SOC_deviation: 6.1472 Fuel Consumption: 48.7060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -112.05844866170426 Explore P: 0.0186 SOC: 0.6146 Cumulative_SOC_deviation: 6.3504 Fuel Consumption: 48.5545\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -113.67610390125856 Explore P: 0.0183 SOC: 0.6056 Cumulative_SOC_deviation: 6.5708 Fuel Consumption: 47.9680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -112.10955474731209 Explore P: 0.0181 SOC: 0.6032 Cumulative_SOC_deviation: 6.4708 Fuel Consumption: 47.4011\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -112.94698383937265 Explore P: 0.0179 SOC: 0.6070 Cumulative_SOC_deviation: 6.6113 Fuel Consumption: 46.8341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -115.83646492280758 Explore P: 0.0177 SOC: 0.6045 Cumulative_SOC_deviation: 6.8738 Fuel Consumption: 47.0980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -112.91735672967093 Explore P: 0.0175 SOC: 0.6053 Cumulative_SOC_deviation: 6.5795 Fuel Consumption: 47.1225\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -117.94562840866136 Explore P: 0.0173 SOC: 0.6038 Cumulative_SOC_deviation: 7.0242 Fuel Consumption: 47.7041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -125.6534139897424 Explore P: 0.0171 SOC: 0.6050 Cumulative_SOC_deviation: 7.7588 Fuel Consumption: 48.0658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -140.94792033259685 Explore P: 0.0169 SOC: 0.6042 Cumulative_SOC_deviation: 9.2828 Fuel Consumption: 48.1195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -107.5809920195119 Explore P: 0.0167 SOC: 0.6059 Cumulative_SOC_deviation: 5.9711 Fuel Consumption: 47.8703\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -106.76106049748513 Explore P: 0.0165 SOC: 0.6078 Cumulative_SOC_deviation: 5.8412 Fuel Consumption: 48.3492\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -106.365695116277 Explore P: 0.0163 SOC: 0.6057 Cumulative_SOC_deviation: 5.7880 Fuel Consumption: 48.4861\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -108.4782858202962 Explore P: 0.0162 SOC: 0.6096 Cumulative_SOC_deviation: 6.0139 Fuel Consumption: 48.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -110.58241062112232 Explore P: 0.0160 SOC: 0.6042 Cumulative_SOC_deviation: 6.2888 Fuel Consumption: 47.6943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -103.85098490570293 Explore P: 0.0158 SOC: 0.6043 Cumulative_SOC_deviation: 5.6968 Fuel Consumption: 46.8830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -108.15324615414036 Explore P: 0.0157 SOC: 0.6050 Cumulative_SOC_deviation: 6.1383 Fuel Consumption: 46.7706\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -108.83104400435671 Explore P: 0.0155 SOC: 0.6047 Cumulative_SOC_deviation: 6.2315 Fuel Consumption: 46.5164\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -103.26949633897601 Explore P: 0.0154 SOC: 0.6085 Cumulative_SOC_deviation: 5.5214 Fuel Consumption: 48.0560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -110.3093087833973 Explore P: 0.0152 SOC: 0.6029 Cumulative_SOC_deviation: 6.3123 Fuel Consumption: 47.1860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -108.48093918328043 Explore P: 0.0151 SOC: 0.6060 Cumulative_SOC_deviation: 6.0992 Fuel Consumption: 47.4890\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -106.4030196290844 Explore P: 0.0149 SOC: 0.6043 Cumulative_SOC_deviation: 5.9197 Fuel Consumption: 47.2056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -111.38979504750854 Explore P: 0.0148 SOC: 0.6082 Cumulative_SOC_deviation: 6.4067 Fuel Consumption: 47.3229\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -108.01516963138518 Explore P: 0.0147 SOC: 0.6089 Cumulative_SOC_deviation: 5.9456 Fuel Consumption: 48.5594\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -106.83809625078766 Explore P: 0.0146 SOC: 0.6028 Cumulative_SOC_deviation: 5.8919 Fuel Consumption: 47.9191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -111.25618064506651 Explore P: 0.0144 SOC: 0.6065 Cumulative_SOC_deviation: 6.3674 Fuel Consumption: 47.5819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -113.4628931201538 Explore P: 0.0143 SOC: 0.6064 Cumulative_SOC_deviation: 6.5847 Fuel Consumption: 47.6161\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -106.74073572888206 Explore P: 0.0142 SOC: 0.6032 Cumulative_SOC_deviation: 5.8831 Fuel Consumption: 47.9094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -108.89024269390332 Explore P: 0.0141 SOC: 0.6057 Cumulative_SOC_deviation: 6.1509 Fuel Consumption: 47.3815\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_3_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"DDQN3_3.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
